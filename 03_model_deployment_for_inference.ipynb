{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c62cc95-69e4-4820-9a0c-e605ad87b25e",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/combined_logo.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0863b8-d8e1-4ae8-88d8-154e69e14de1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Disaster Risk Monitoring Using Satellite Imagery #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e3b9e-1612-460a-a3b1-1560f0651441",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 03 - Deploying a Model for Inference ##\n",
    "In this notebook, you will take the previously trained segmentation model and deploy it with Triton Inference Server. You'll learn how to create the model directory structures and configuration files within Triton Inference Server as well as how to send inference requests. \n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This notebook covers the below sections: \n",
    "1. [Model Deployment](#s3-1)\n",
    "2. [Introduction to Triton Inference Server](#s3-2)\n",
    "    * [Server](#s3-2.1)\n",
    "    * [Client](#s3-2.2)\n",
    "3. [Model Repository](#s3-3)\n",
    "    * [Exercise #1 - Model Configuration](#s3-e1)\n",
    "4. [Run Inference on Triton Inference Server](#s3-4)\n",
    "    * [Server Health Status](#s3-4.1)\n",
    "    * [Exercise #2 - Pre-Process Inputs](#s3-e2)\n",
    "    * [Send Inference Request to Server](#s3-4.2)\n",
    "    * [Visualize Results](#s3-4.3)\n",
    "    * [Run Batch Inference](#s3-4.4)\n",
    "5. [Inference Performance Optimization](#s3-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790bbf1e-fe18-46d5-b303-f725dda5af84",
   "metadata": {},
   "source": [
    "<a name='s3-1'></a>\n",
    "## Model Deployment ##\n",
    "Model deployment refers to the integration of deep learning inference into an existing system or process. Sometimes it's advantageous to keep the deep learning model isolated, and other times, it needs to be more available for broad use. We imagine our flood detection model to have more utility if it can produce real-time inference, therefore we employ a solution to allow for its use via network APIs. While model training focuses on accuracy as the key metric, model deployment concentrates on inference performance metrics such as [throughput](https://en.wikipedia.org/wiki/Network_throughput), [latency](https://en.wikipedia.org/wiki/Latency_(engineering)), and hardware utilization. \n",
    "\n",
    "Model deployment considerations: \n",
    "* **Throughput**: the amount of data that are processed in a specific amount of time\n",
    "* **Latency**: the amount of time it takes for the input to be processed is critical for real-time inference\n",
    "* **Hardware Utilization**: how much of the hardware is being utilized/unutilized\n",
    "* **Deployment Environment**: environments can vary depending on the application - public cloud, on-premises core (data center), enterprise edge, and on embedded devices. \n",
    "\n",
    "In the context of a disaster risk monitoring system, we should consider optimizing throughput at a low cost for processing satellite images to create high fidelity maps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1945095-75e2-4b72-b47c-d30ba6003d67",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s3-2'></a>\n",
    "## Introduction to Triton Inference Server ##\n",
    "NVIDIA [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) simplifies the deployment of AI models at scale in production. It's an open-source, inference-serving software that lets teams deploy trained AI models from any framework, from local storage, or from cloud service providers on any GPU or CPU-based infrastructure, cloud, data center, or edge. The below figure shows the Triton Inference Server's high-level architecture. The model repository is a _file-system based repository_ of the models that Triton Inference Server will make available for inferencing. Inference requests arrive at the server via either [HTTP/REST](https://en.wikipedia.org/wiki/Representational_state_transfer), [gRPC](https://en.wikipedia.org/wiki/GRPC), or by the C API and are then routed to the appropriate per-model scheduler. Triton Inference Server implements multiple scheduling and batching algorithms that can be configured on a model-by-model basis. Each model's scheduler optionally performs batching of inference requests and then passes the requests to the backend corresponding to the model type. The backend performs inferencing using the inputs provided in the batched requests to produce the requested outputs. The outputs are then returned.\n",
    "<p><img src='images/triton_server_architecture.png' width='720'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b5277-b33b-4a1e-96bb-1b5570b60647",
   "metadata": {},
   "source": [
    "<a name='s3-2.1'></a>\n",
    "### Server ###\n",
    "Setting up the Triton Inference Server requires software for the server and the client. One can get started with Triton Inference Server by pulling the [container](https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver) from the NVIDIA NGC catalog. In this lab, we already have Triton Inference Server instance running. More details can be found in the [QuickStart Documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/quickstart.md) and [Build Documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/build.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b5615-d578-4fb8-8f61-d4eca4afa891",
   "metadata": {},
   "source": [
    "<a name='s3-2.2'></a>\n",
    "### Client ###\n",
    "We've also installed the Triton Inference Server Client libraries to provide APIs that make it easy to communicate with Triton from your C++ or Python application. Using these libraries, you can send either HTTP/REST or gRPC requests to Triton Inference Server to access all its capabilities: inferencing, status and health, statistics and metrics, model repository management, etc. These libraries also support using system and CUDA shared memory for passing inputs to and receiving outputs from Triton Inference Server. For more details on how to download or build the Triton Inference Server Client libraries, you can find the documentation [here](https://github.com/triton-inference-server/server/blob/r20.12/docs/client_libraries.md), as well as examples that show the use of both the C++ and Python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a84b9-2dd7-4ddb-8333-3add28b986ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s3-3'></a>\n",
    "## Model Repository ##\n",
    "Triton Inference Server serves models within a model repository. When you first run Triton Inference Server, you'll specify the model repository where the models reside:\n",
    "```\n",
    "tritonserver --model-repository=/models\n",
    "```\n",
    "Each model resides in its own model subdirectory within the model repository - i.e., each directory within `/models` represents a unique model. For example, in this notebook we'll be deploying our `flood_segmentation_model`. All models typically follow a similar directory structure. Within each of these directories, we'll create a configuration file `config.pbtxt` that details information about the model - e.g. _batch size_, _input shapes_, _deployment backend_ (PyTorch, ONNX, TensorFlow, TensorRT, etc.) and more. Additionally, we can create one or more versions of our model. Each version lives under a subdirectory name with the respective version number, starting with `1`. It is within this subdirectory where our model files reside. For more details on how to work with model repositories and model directory structures in Triton Inference Server, please see the [documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/model_repository.md). \n",
    "\n",
    "Here is an example structure of the model repository that contains 2 different models. \n",
    "\n",
    "```\n",
    "root@server:/models$ tree\n",
    ".\n",
    "├── flood_segmentation_model\n",
    "│   ├── 1\n",
    "│   │   └── model.plan\n",
    "│   └── config.pbtxt\n",
    "│\n",
    "├── flood_segmentation_model_batch\n",
    "│   ├── 1\n",
    "│   │   └── model.plan\n",
    "│   └── config.pbtxt\n",
    "```\n",
    "\n",
    "We can also add a file representing the names of the outputs. We have omitted this step in this notebook for the sake of brevity. For more details on how to work with model repositories and model directory structures in Triton Inference Server, please see the [documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/model_repository.md). Below we'll create the model directory structure for our flood detection segmentation model and copy the exported TensorRT engine into the model repository. For demonstration purposes, we have prepared a sample ResNet18 model (`sample_resnet18.engine`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625ce41-6668-4ddc-bee2-a57f71aa11d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# set environment variables\n",
    "import os\n",
    "\n",
    "%set_env LOCAL_PROJECT_DIR=/dli/task/tao_project\n",
    "%set_env LOCAL_DATA_DIR=/dli/task/flood_data\n",
    "\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"]=os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\"), \"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6839209-7b45-4972-8eb7-4f2adda8fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# create directory for model\n",
    "!mkdir -p models/flood_segmentation_model/1\n",
    "\n",
    "# copy sample_resnet18.engine to the model repository\n",
    "!cp $LOCAL_EXPERIMENT_DIR/export/sample_resnet18.engine models/flood_segmentation_model/1/model.plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f1991-163c-4e2b-b619-207d1d0dfbc4",
   "metadata": {},
   "source": [
    "<a name='s3-e1'></a>\n",
    "### Exercise #1 - Model Configuration ###\n",
    "With our model directory set up, we now turn our attention to creating the configuration file for our model. A minimal model configuration must specify the `name` of the model, the `platform` and/or backend properties, the `max_batch_size` property, and the `input` and `output` tensors of the model (name, data type, and shape). We can get the `output` tensor name from the `nvinfer_config.txt` [file](tao_project/unet/resnet18/weights/nvinfer_config.txt) we generated before under `output-blob-names`. For more details on how to create model configuration files within Triton Inference Server, please see the [documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/model_configuration.md). \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `<FIXME>`s only and execute the below cell to create the `config.pbtxt` file for the segmentation model. \n",
    "* Execute the cell below to show the structure of the model repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277f729-7911-4238-ae6f-07afd708acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"flood_segmentation_model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input: [\n",
    " {\n",
    "    name: \"input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, <<<<FIXME>>>>, <<<<FIXME>>>> ]\n",
    "  }\n",
    "]\n",
    "output: {\n",
    "    name: \"<<<<FIXME>>>>\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ <<<<FIXME>>>>, <<<<FIXME>>>>, 1 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/flood_segmentation_model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb118d-e446-48ed-a741-3a92de63c38d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# show model repository folder structure\n",
    "!tree -a models"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae59c112-b5ac-4096-985b-86cb10259e98",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "configuration = \"\"\"\n",
    "name: \"flood_segmentation_model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input: [\n",
    " {\n",
    "    name: \"input_1:0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 512, 512 ]\n",
    "  }\n",
    "]\n",
    "output: {\n",
    "    name: \"argmax_1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 512, 512, 1 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/flood_segmentation_model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30792e9f-97da-4c79-8bed-ef33b87a4215",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39ba239-eb7d-4c72-b8c7-9b70074ba545",
   "metadata": {},
   "source": [
    "<a name='s3-4'></a>\n",
    "## Run Inference on Triton Inference Server ##\n",
    "With our model directory structures created, models defined and exported, and configuration files created, we will now wait for Triton Inference Server to load our models. We have set up this lab to use Triton Inference Server in **polling** mode. This means that Triton Inference Server will continuously poll for modifications to our models or for newly created models - once every 30 seconds. Please run the cell below to allow time for Triton Inference Server to poll for new models/modifications before proceeding. Due to the asynchronous nature of this step, we have added 15 seconds to be safe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3800be-ce51-4bcc-a353-fdcf03806f21",
   "metadata": {},
   "source": [
    "<a name='s3-4.1'></a>\n",
    "### Server Health Status ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4645417-1f93-4456-b6e3-0c7c2e7b3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49aeb80-4b61-43c9-a194-f4dc0ad00e02",
   "metadata": {},
   "source": [
    "At this point, our models should be deployed and ready to use! To confirm Triton Inference Server is up and running, we can send a `curl` request to the below URL. The HTTP request returns status _200_ if Triton is ready and _non-200_ if it is not ready. We can also send a `curl` request to our model endpoints to confirm our models are deployed and ready to use. Additionally, we will also see information about our models such:\n",
    "* The name of our model\n",
    "* The versions available for our model\n",
    "* The backend platform (e.g. tensort_rt, pytorch_libtorch, onnxruntime_onnx)\n",
    "* The inputs and outputs, with their respective names, data types, and shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7eb618-7d6b-48d2-8937-2a8b37ab072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626d1a8-ba1c-446c-a53d-891ed78d1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!curl -v triton:8000/v2/models/flood_segmentation_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93558b9e-43d7-4d2a-8f9f-3a87129f7ca5",
   "metadata": {},
   "source": [
    "<a name='s3-4.2'></a>\n",
    "### Send Inference Request to Server ###\n",
    "With our models deployed, it is now time to send inference requests to our models. Triton Inference Server itself does not do anything with your input tensors, it simply feeds them to the model. Same for outputs. Ensuring that the pre-processing operations used for inference are defined identically as they were when the model was trained is key to achieving high accuracy. \n",
    "\n",
    "<p><img src='images/inference_pipeline.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62621e9b-92b0-432b-8ef2-eef7dd299432",
   "metadata": {},
   "source": [
    "<a name='s3-e2'></a>\n",
    "### Exercise #2 - Pre-Process Inputs ###\n",
    "When we developed our segmentation model with the TAO Toolkit, some data pre-processing were done for training. Therefore, we need to perform the same normalization and mean subtraction to produce the final float planar data that the TensorRT engine is looking for. We can get the `offsets` and `net-scale-factor` from the `nvinfer_config.txt` [file](tao_project/unet/resnet18/weights/nvinfer_config.txt). The pre-processing function is:\n",
    "\n",
    "<b>y = net scale factor * (x-mean)</b>\n",
    "\n",
    "where: \n",
    "* **x** is the input pixel value. It is an int8 with range [0,255].\n",
    "* **mean** is the corresponding mean value, read either from the mean file or as offsets[c]. Here c is the channel to which the input pixel belongs and offsets is the array specified in the configuration file. In the case of `sample_resnet18.engine`, it is `127.5`. \n",
    "* **net-scale-factor** is the pixel scaling factor specified in the configuration file. In the case of `sample_resnet18.engine`, it is `1/127.5`, or `0.00784313725490196`. \n",
    "* **y** is the corresponding output pixel value. It is a float.\n",
    "\n",
    "**Instructions**:<br>\n",
    "* Execute the below cell to import dependencies. \n",
    "* Modify the `<FIXME>`s only and execute the cell below to define the `preprocess_image` function. \n",
    "* Execute the cell below to perform pre-processing on a random image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df2234-c035-4200-9a0a-5e0e7772de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a56d3-4a2c-430e-be9b-9b0a00b2d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pre-process function\n",
    "def preprocess_image(image): \n",
    "    \"\"\"\n",
    "    This function returns the pre-processed image as a NumPy array. \n",
    "    \"\"\"\n",
    "    # load image as a 32-bit floating point array \n",
    "    image_ary=np.asarray(image)\n",
    "    image_ary=image_ary.astype(np.float32)\n",
    "    \n",
    "    # pre-processing\n",
    "    image_ary=(image_ary-<<<<FIXME>>>>)*<<<<FIXME>>>>\n",
    "    \n",
    "    # unet segmentation model requires the data to be in BGR format\n",
    "    BGR=np.empty_like(image_ary)\n",
    "    BGR[:, :, 0]=image_ary[:, :, 2]\n",
    "    BGR[:, :, 1]=image_ary[:, :, 1]\n",
    "    BGR[:, :, 2]=image_ary[:, :, 0]\n",
    "    image_ary=BGR\n",
    "    \n",
    "    # convert array from h, w, c to 1, c, h, w\n",
    "    image_ary=np.transpose(image_ary, [2, 0, 1])\n",
    "    image_ary=np.expand_dims(image_ary, axis=0)\n",
    "    return image_ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2b479-64d4-46d7-bb0c-0abf346a2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# choose random image\n",
    "random_image_file=random.sample(os.listdir(os.path.join(os.getenv('LOCAL_DATA_DIR'), 'images', 'all_images')), 1)[0]\n",
    "\n",
    "# preprocess\n",
    "image=Image.open(os.path.join(os.getenv('LOCAL_DATA_DIR'), 'images', 'all_images', random_image_file))\n",
    "mask=Image.open(os.path.join(os.getenv('LOCAL_DATA_DIR'), 'masks', 'all_masks', random_image_file))\n",
    "image_ary=preprocess_image(image)\n",
    "\n",
    "print('The input array has a shape of {}.'.format(image_ary.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb1a751a-1967-4944-876d-c34084397fc8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def preprocess_image(input_image): \n",
    "    image_ary=np.asarray(input_image)\n",
    "    image_ary=image_ary.astype(np.float32)\n",
    "    \n",
    "    image_ary=(image_ary-127.5)*0.00784313725490196\n",
    "    \n",
    "    BGR=np.empty_like(image_ary)\n",
    "    BGR[:, :, 0]=image_ary[:, :, 2]\n",
    "    BGR[:, :, 1]=image_ary[:, :, 1]\n",
    "    BGR[:, :, 2]=image_ary[:, :, 0]\n",
    "    image_ary=BGR\n",
    "    \n",
    "    image_ary=np.transpose(image_ary, [2, 0, 1])\n",
    "    image_ary=np.expand_dims(image_ary, axis=0)\n",
    "    return image_ary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10ef11-8edd-48a0-9da1-af960b5070a4",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7798c8-4aee-47db-8fa7-c3abe2cfc829",
   "metadata": {},
   "source": [
    "Next, we'll load the `tritonclient.http` module. We will also define the input and output names of our model, the name of our model, the URL where our models are deployed with Triton Inference Server (in this case the host `triton:8000`), and our model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a18b3-ecdc-45c9-95b2-a5d8d3681646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "import tritonclient.http as tritonhttpclient\n",
    "from pprint import pprint\n",
    "\n",
    "# set parameters\n",
    "VERBOSE=False\n",
    "input_name='input_1:0'\n",
    "input_shape=(1, 3, 512, 512)\n",
    "input_dtype='FP32'\n",
    "output_name='argmax_1'\n",
    "model_name='flood_segmentation_model'\n",
    "url='triton:8000'\n",
    "model_version='1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d03260-a4e7-4668-8cd8-0d88a371562b",
   "metadata": {},
   "source": [
    "We'll instantiate our client `triton_client` using the `tritonhttpclient.InferenceServerClient` class and access the model metadata with the `get_model_metadata` method as well as get our model configuration with the `get_model_config` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f84f5-0c7a-4169-aa78-81283c2bf59a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# instantiate Triton Inference Server client\n",
    "triton_client=tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "\n",
    "# get model metadata\n",
    "print('----------Metadata----------')\n",
    "model_metadata=triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "pprint(model_metadata)\n",
    "\n",
    "# get model configuration\n",
    "print('----------Configuration----------')\n",
    "model_config=triton_client.get_model_config(model_name=model_name, model_version=model_version)\n",
    "pprint(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ca85d-f46f-46bc-b0b0-27a965ebb4c8",
   "metadata": {},
   "source": [
    "We'll instantiate a placeholder for our input data using the expected input name, shape, and data type. We'll set the data of the input to be the NumPy array representation of our image. We'll also instantiate a placeholder for our output data using just the output name. Lastly, we'll submit our input to the Triton Inference Server using the `triton_client.infer` method, specifying our model name, model version, inputs, and outputs and convert our result to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4884f-3187-4725-972b-f26aa5e80758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "inference_input=tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "output=tritonhttpclient.InferRequestedOutput(output_name)\n",
    "\n",
    "inference_input.set_data_from_numpy(image_ary)\n",
    "\n",
    "# time the process\n",
    "start=time.time()\n",
    "response=triton_client.infer(model_name, \n",
    "                             model_version=model_version, \n",
    "                             inputs=[inference_input], \n",
    "                             outputs=[output])\n",
    "latency=time.time()-start\n",
    "logits=response.as_numpy(output_name)\n",
    "\n",
    "print('The output array has a shape of {}.'.format(logits.shape))\n",
    "print('It took {} per inference.'.format(round(latency, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425b3d9-eefe-4350-8fb5-e2471bbc7d25",
   "metadata": {},
   "source": [
    "<a name='s3-4.3'></a>\n",
    "### Visualize Results ###\n",
    "Since the last layer of our segmentation model is a [SoftMax](https://en.wikipedia.org/wiki/Softmax_function) layer, we can identify the largest logit value per pixel and visually confirm that our model inferred the presence of flood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db686646-4d0f-49ee-88bf-29f39588c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# visualize results\n",
    "fig, ax_arr=plt.subplots(1, 3, figsize=[15, 5], sharex=True, sharey=True)\n",
    "ax_arr[0].set_title('Input Data')\n",
    "ax_arr[1].set_title('Inference')\n",
    "ax_arr[2].set_title('Actual')\n",
    "ax_arr[0].set_xticks([])\n",
    "ax_arr[0].set_yticks([])\n",
    "\n",
    "ax_arr[0].imshow(image)\n",
    "ax_arr[1].imshow(logits[0], cmap='gray')\n",
    "ax_arr[2].imshow(mask, cmap='gray')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc05b4c-c28c-4438-8522-871fd3b71aca",
   "metadata": {},
   "source": [
    "<p><img src='images/tip.png' width=720></p>\n",
    "\n",
    "Recall that we used a random input image for demonstration. In case you don't get an image that shows presence of flood, please feel free to re-run the inference starting from [Exercise #2](#s3-e2). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb8b62-8181-438a-9f53-31f9adaa5638",
   "metadata": {},
   "source": [
    "We can iterate through all input images to see how quickly Triton is able to perform inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11299f69-796b-4412-94e6-b472035d942b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # DO NOT CHANGE THIS CELL\n",
    "time_list=[]\n",
    "\n",
    "for image_path in os.listdir(os.path.join(os.getenv('LOCAL_DATA_DIR'), 'images', 'all_images')): \n",
    "    image=Image.open(os.path.join(os.getenv('LOCAL_DATA_DIR'), 'images', 'all_images', image_path))\n",
    "    image_ary=preprocess_image(image)\n",
    "    inference_input.set_data_from_numpy(image_ary)\n",
    "    \n",
    "    # time the process\n",
    "    start=time.time()\n",
    "    response=triton_client.infer(model_name, \n",
    "                                 model_version=model_version, \n",
    "                                 inputs=[inference_input], \n",
    "                                 outputs=[output])\n",
    "    time_list.append(time.time()-start)\n",
    "    logits=response.as_numpy(output_name)\n",
    "    \n",
    "latency=sum(time_list)/len(time_list)\n",
    "print('It took {} seconds to infer {} images.'.format(round(sum(time_list), 3), len(time_list)))\n",
    "print('On average it took {} seconds per inference.'.format(round(latency, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23db51-83e6-4412-be86-1a0ca429c68d",
   "metadata": {},
   "source": [
    "<a name='s3-4.4'></a>\n",
    "### Run Batch Inference ###\n",
    "One factor to consider for inference optimization is batch size, or how many samples you process at once. Below we're going to perform inference in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e36f0-e82e-45cf-ac3e-f5cecb79d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41627b3-fa85-4b58-a978-325b7ff6bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# create directory for model\n",
    "!mkdir -p models/flood_segmentation_model_batch/1\n",
    "\n",
    "# copy sample_resnet18.engine to the model repository\n",
    "!cp $LOCAL_EXPERIMENT_DIR/export/sample_resnet18.engine models/flood_segmentation_model_batch/1/model.plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e03abd-7a88-4080-b0d2-704bb08f37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "configuration = \"\"\"\n",
    "name: \"flood_segmentation_model_batch\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: {}\n",
    "input: [\n",
    " {{\n",
    "    name: \"input_1:0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 512, 512 ]\n",
    " }}\n",
    "]\n",
    "output: {{\n",
    "    name: \"argmax_1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 512, 512, 1]\n",
    " }}\n",
    "\"\"\".format(batch_size)\n",
    "\n",
    "with open('models/flood_segmentation_model_batch/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f6c2a-d84e-4b39-8aad-76b267dacbab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# show model repository folder structure\n",
    "!tree -a models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57762335-f8fa-40e7-a536-a46c6836dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!sleep 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073cc3db-eb3c-4379-8719-071c4b19ee12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!curl -v triton:8000/v2/models/flood_segmentation_model_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac0999-ab6d-4c69-84ce-408ad0e4d67a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define new input shape\n",
    "batch_input_shape=(batch_size, 3, 512, 512)\n",
    "\n",
    "batch_inference_input=tritonhttpclient.InferInput(name='input_1:0', shape=batch_input_shape, datatype='FP32')\n",
    "batch_output=tritonhttpclient.InferRequestedOutput('argmax_1')\n",
    "\n",
    "# create empty array for the batch input\n",
    "batch_ary=np.empty(batch_input_shape).astype(np.float32)\n",
    "\n",
    "time_list=[]\n",
    "\n",
    "# iterate through all images\n",
    "for idx, image_path in enumerate(os.listdir(os.path.join(os.getenv('LOCAL_DATA_DIR'), 'images', 'all_images'))): \n",
    "    image=Image.open(os.path.join(os.getenv('LOCAL_DATA_DIR'), 'images', 'all_images', image_path))\n",
    "    batch_ary[idx%batch_size]=preprocess_image(image)\n",
    "    if idx%batch_size==(batch_size-1): \n",
    "        batch_inference_input.set_data_from_numpy(batch_ary)\n",
    "\n",
    "        # time the process\n",
    "        start=time.time()\n",
    "        response=triton_client.infer(model_name='flood_segmentation_model_batch', \n",
    "                                     model_version='1', \n",
    "                                     inputs=[batch_inference_input], \n",
    "                                     outputs=[batch_output])\n",
    "        time_list.append(time.time()-start)\n",
    "        logits=response.as_numpy(output_name)\n",
    "\n",
    "batch_latency=sum(time_list)/len(time_list)\n",
    "print('It took {} seconds to infer {} images.'.format(round(sum(time_list), 3), len(time_list)*batch_size))\n",
    "print('On average it took {} seconds per inference.'.format(round(batch_latency, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86195126-f9a5-4a49-afc0-84f4101c66d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s3-5'></a>\n",
    "## Inference Performance Optimization ##\n",
    "There is usually a trade-off between latency and throughput. Finding the right balance between them usually involves satisfying the application requirements. As an example, the Triton Inference Server can be connected to front-end applications such as those that power https://www.balcony.io/, which provides an emergency management platform. Amongst other services, it has the ability to send alerts to personal devices in real-time. NVIDIA's GPUs deliver high throughput at higher batch size. However, for real-time applications, the real constraint on services isn’t batch size or even throughput, but rather the latency required to deliver an outstanding experience for end users. You can read more about how Triton Inference Server is used to enable rapid disaster detection and response [here](https://siliconvalley.orange.com/en/news/rapid-disaster-detection-and-response-a-climate-action-collaboration/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0621fcd1-3065-4718-a46f-d6bed754b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# plot throughput vs. latency\n",
    "plt.title('Inference Performance Comparison')\n",
    "plt.plot([latency, batch_latency], [1/latency, batch_size/batch_latency], marker='o')\n",
    "plt.text(latency, (1/latency)-2.5, 'Non-Batching')\n",
    "plt.text(batch_latency, (batch_size/batch_latency)-2.5, 'Batching ({})'.format(batch_size))\n",
    "\n",
    "plt.xlabel('Latency (Second)')\n",
    "plt.ylabel('Throughput (Image/Second)')\n",
    "plt.xlim(xmin=0, xmax=max(batch_latency, latency)*1.25)\n",
    "plt.ylim(ymin=0, ymax=max(1/latency, batch_size/batch_latency)*1.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbdaf-79ea-4f38-9667-22d2cc04ce74",
   "metadata": {},
   "source": [
    "**Well Done!** Let's move to the [next notebook](./04_unosat_flood_event_case_study.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dc84b-ccd8-4295-87cf-897090d19c42",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/combined_logo.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
